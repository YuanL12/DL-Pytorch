## 注意点
1. Pytorch 的卷积层`nn.Conv2d(1, 5, kernel_size=3, padding=1)` 需要先指定输入和输出通道：here, 1 for one input channel like grey-image, 5 for 5 output channels 
2. input 的 shape 需要四个维度：`(batch_size, channel, height, width)`
3. padding = 1 means ***up, low, left, right*** all pluse one. 
4. How to compute size: $\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.$, where $(n_h-k_h+p_h+s_h)$ each represents ***input height, kernel height, height padding number = `padding` x 2 , height stride number***.



## 互相关运算，求输出矩阵
```Python 
def corr2d(X, K):
    """Compute 2D cross-correlation."""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```

## 卷积层
Note: There is only one real number as bias for a convotional layer.
```Python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

## 手动训练
Pytorch 的卷积层需要指定，输入、输出通道
同时需要将input-X 和 output-Y reshape成输入输出通道对应的shape
```Python
# Construct a two-dimensional convolutional layer with 1 output channel and a
# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here
conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)

# The two-dimensional convolutional layer uses four-dimensional input and
# output in the format of (example, channel, height, width), where the batch
# size (number of examples in the batch) and the number of channels are both 1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # Learning rate

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y)**2
    conv2d.zero_grad()
    l.sum().backward()
    # Update the kernel
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'batch {i + 1}, loss {l.sum():.3f}')
```
## Padding and Stride填充和步幅
### Padding
```Python
import torch
from torch import nn

# We define a convenience function to calculate the convolutional layer. This
# function initializes the convolutional layer weights and performs
# corresponding dimensionality elevations and reductions on the input and
# output
def comp_conv2d(conv2d, X):
    # Here (1, 1) indicates that the batch size and the number of channels
    # are both 1
    X = X.reshape((1, 1) + X.shape)
	# now X.shape == (1, 1, m, n)
    Y = conv2d(X)
    # Exclude the first two dimensions that do not interest us: examples and
    # channels
    return Y.reshape(Y.shape[2:])

# Note that here 1 row or column is padded on either side, so a total of 2
# rows or columns are added
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
```

kernel: 3x3
input: 8x8
padding: 1 for each edge, i.e., 2 for row and column
output: (8-3+2+1)x(8-3+2+1)= (8,8)

```Python
conv2d = nn.Conv2d(1, 1, kernel_size=(5,3), padding=(2,1))
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
```
kernel: 5x3
input: 8x8
padding: 2 for two ends of row, 1 for two ends of column
output: (8-5+4+1)x(8-3+2+1) = (8,8)

### Stride
```Python
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
```
一般使用对称的填充和步幅。

如何计算size:
$\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.$

## Pooling池化层
目的：可以缓解卷积层对位置的敏感性

- 池化层有填充和步幅
- 但是没有可以学习的参数
- 在每个输入通道应用池化层，以获得相应的输出通道。
- 输入通道数=输出通道数

```Python
pool2d = nn.MaxPool2D(3)
pool2d(X)  # 因为池化层没有模型参数，所以不需要调用参数初始化函数

pool2d = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))
pool2d(X)
```